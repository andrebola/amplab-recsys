{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_predict\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from analyze_similars import show_similar\n",
    "from scipy import sparse\n",
    "from generate_mtrx import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims=200\n",
    "model_folder = 'models'\n",
    "split_folder='lastfm'\n",
    "user_features_filename = 'out_user_features_{}.feats'\n",
    "item_features_filename = 'out_item_features_{}.feats'\n",
    "predictions_filename = 'predicted_{}.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split\n",
    "\n",
    "We start by spliting the dataset in train and test. The split method receives a parameter to indicate the percentage that will be in train and in test, in this case we use 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300000it [00:00, 911381.23it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_location = 'data/usersha1-artmbid-artname-plays-part1.tsv'\n",
    "fan_train, fan_test_data, fan_items_dict, fan_users_dict, fan_item_ids = split(0.2, dataset_location)\n",
    "fan_train = fan_train.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Matrix Factorization model\n",
    "\n",
    "The next step is to train the model, for that we need to specify the dimension to use in the representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_file = os.path.join(model_folder, split_folder, user_features_filename)\n",
    "item_features_file = os.path.join(model_folder, split_folder, item_features_filename)\n",
    "\n",
    "item_ids, item_vecs_reg, user_ids, user_vecs_reg = model_predict.train_als(fan_train, dims, fan_users_dict, fan_items_dict, user_features_file, item_features_file, save_res=False)\n",
    "#user_ids, user_vecs_reg = model_predict.load_feats(user_features_file)\n",
    "#item_ids, item_vecs_reg = model_predict.load_feats(item_features_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute predictions\n",
    "\n",
    "After obtaining the representations of the users and the items we can make the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_file = os.path.join(model_folder, split_folder,predictions_filename)\n",
    "predicted = model_predict.predict(item_vecs_reg, user_vecs_reg, predictions_file, fan_train, step=500)\n",
    "#predicted = np.load(predictions_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "The next step is to evaluate the results, for that we compare the recomendations with the set set. In this case we use MAP@10, Precision @ 1,3,5,10 r-precision and nDCG@10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map@10': 0.0732971508192089, 'precision@1': 0.23042579072039548, 'precision@3': 0.19046412867896412, 'precision@5': 0.1683572667225221, 'precision@10': 0.13602367081314143, 'r-precision': 0.13726744757201773, 'ndcg@10': 0.1606607908605957}\n"
     ]
    }
   ],
   "source": [
    "model_predict.show_eval(predicted, fan_test_data, item_ids, fan_train, fan_item_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the recommendations\n",
    "\n",
    "Now we take one user (i=10) and we compare what the system recommends with what the user listened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "Listened (test) ['mew', 'kent', 'the sounds', 'hot chip', 'r.e.m.', 'franz ferdinand', 'oasis', 'timo räisänen', 'placebo', 'lars winnerbäck', 'detektivbyrån']\n",
      "---------\n",
      "Listened (train) ['the killers', 'the strokes', 'arcade fire', 'patrick wolf', 'arctic monkeys', 'death cab for cutie', 'coldplay', 'radiohead', 'muse', 'snow patrol', 'the decemberists', 'the beatles', 'madonna', 'hoobastank', 'panic at the disco', 'markus krunegård', 'millencolin', 'tegan and sara', 'säkert!', 'raymond & maria', 'the ark', 'alice in videoland', 'the kooks', 'sahara hotnights', 'david fridlund', 'the knife', 'keane', 'håkan hellström', 'kings of convenience', 'the tough alliance', 'david & the citizens', 'shout out louds', 'slagsmålsklubben', 'mando diao', 'tiger lou', 'the cardigans', 'vapnet', 'bloc party', 'laleh', 'green day', 'deportees', 'firefox ak']\n",
      "---------\n",
      "Recommended [('franz ferdinand', True), ('kent', True), ('the shins', False), ('detektivbyrån', True), ('timo räisänen', True), ('broder daniel', False), ('laakso', False), ('moneybrother', False), ('johnossi', False), ('kristian anttila', False)]\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "model_predict.show_recs(predicted, fan_test_data, item_ids, fan_train, fan_item_ids, i=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artists similarity\n",
    "\n",
    "Finally we compute the similarity of the artists using the original placounts matrix. In this case we show the difference of using a euclidean distance with consine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {k:v for v,k in enumerate(fan_item_ids)}\n",
    "artist_name = 'the beatles'\n",
    "position = [i for i,n in enumerate(fan_item_ids) if n == artist_name][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARS ['the beatles', 'john lennon', 'paul mccartney', 'george harrison', 'the who', 'the kinks', 'simon & garfunkel', 'the pillbugs', 'creedence clearwater revival', 'ok go']\n",
      "SIMILARS ['the beatles', 'john lennon', 'paul mccartney', 'george harrison', 'the who', 'led zeppelin', 'ringo starr', 'the rutles', 'bob dylan', 'paul mccartney & wings']\n"
     ]
    }
   ],
   "source": [
    "show_similar(fan_train, fan_item_ids, names, position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fan_train = sparse.load_npz(os.path.join('data', split_folder, 'fan_train_data.npz')).tocsr()\n",
    "fan_test_data = pickle.load(open(os.path.join('data', split_folder, 'fan_test_data.pkl'), 'rb'))\n",
    "fan_items_dict = pickle.load(open(os.path.join('data', split_folder, 'fan_items_dict.pkl'), 'rb'))\n",
    "item_ids= pickle.load(open(os.path.join('data', split_folder, 'fan_item_ids.pkl'), 'rb'))\n",
    "fan_users_dict = pickle.load(open(os.path.join('data', split_folder,'fan_users_dict.pkl'), 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
